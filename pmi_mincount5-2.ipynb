{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "etMQnHaAFwxS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd6GPictWulr"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "import nltk\n",
        "from scipy.sparse import lil_matrix, coo_matrix\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy.stats import spearmanr\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import time\n",
        "import pandas as pd\n",
        "from scipy.sparse import save_npz\n",
        "from scipy.sparse import load_npz\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from google.colab import drive\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from numpy.linalg import norm\n",
        "!pip install -U datasets\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load stopwords"
      ],
      "metadata": {
        "id": "4BIZ-4rnGEh1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0P1GRWkZN1H"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Methods"
      ],
      "metadata": {
        "id": "ReQ-JetNGPPl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yEX-Gh3XBaP"
      },
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "  # put all text in lowercase so case does not matter\n",
        "  text = text.lower()\n",
        "  # replace anything that is not a lowercase character or whitespace\n",
        "  text = re.sub(r'[^a-z\\s]', '', text)\n",
        "  # split the text into a list of strings (words)\n",
        "  words = text.split()\n",
        "  # remove stopwords\n",
        "  return [word for word in words if word not in stop_words]\n",
        "\n",
        "def build_vocabulary(corpus, min_count=5):\n",
        "  c = Counter()\n",
        "  for sentence, _ in tqdm(corpus, desc=\"Building vocab\"):\n",
        "    c.update(preprocess(sentence))\n",
        "  # filter words by min_count\n",
        "  filtered_words = [word for word, count in c.items() if count >= min_count]\n",
        "  # add a word to the vocabulary only if it matches or exceeds the minimum number of appearances (default = 5)\n",
        "  vocabulary = {word: i for i, word in enumerate(filtered_words)}\n",
        "  return vocabulary\n",
        "\n",
        "def build_matrix(corpus, vocab, window_size=2):\n",
        "  vocab_size = len(vocab)\n",
        "  # create a vocab_size x vocab_size matrix\n",
        "  matrix = lil_matrix((vocab_size, vocab_size), dtype=np.float32)\n",
        "  # set counts for all words in vocab to 0\n",
        "  counts = np.zeros(vocab_size, dtype=np.int32)\n",
        "  tot = 0\n",
        "\n",
        "  for sentence, _ in tqdm(corpus, desc='Preprocessing sentences'):\n",
        "    # split the sentence into a list of words\n",
        "    words = preprocess(sentence)\n",
        "    # get the indices for all words in vocabulary\n",
        "    indices = [vocab[word] for word in preprocess(sentence) if word in vocab]\n",
        "\n",
        "    # for each word in the sentence, add 1 for the words within the window\n",
        "    for i, id in enumerate(indices):\n",
        "      counts[id] += 1\n",
        "      start = max(i - window_size, 0)\n",
        "      end = min(i + window_size + 1, len(indices))\n",
        "\n",
        "      for j in range(start, end):\n",
        "        if i != j:\n",
        "          matrix[id, indices[j]] += 1\n",
        "          tot += 1\n",
        "\n",
        "  return matrix.tocoo(), counts, tot\n",
        "\n",
        "def build_pmi(matrix, counts, tot, positive=True):\n",
        "  rows, cols = matrix.row, matrix.col\n",
        "  data = matrix.data\n",
        "  pmi_data = []\n",
        "\n",
        "  for i in tqdm(range(len(data)), desc=\"Computing PMI\"):\n",
        "    w1 = rows[i]\n",
        "    w2 = cols[i]\n",
        "    p_w1w2 = data[i]/tot\n",
        "    p_w1 = counts[w1]/tot\n",
        "    p_w2 = counts[w2]/tot\n",
        "\n",
        "    if p_w1 > 0 and p_w2 > 0 and p_w1w2 > 0:\n",
        "      pmi = math.log2(p_w1w2 / (p_w1 * p_w2))\n",
        "\n",
        "      if positive:\n",
        "        pmi = max(pmi, 0)\n",
        "      pmi_data.append(pmi)\n",
        "    else:\n",
        "      pmi_data.append(0)\n",
        "\n",
        "  pmi_matrix = coo_matrix((pmi_data, (rows, cols)), shape=matrix.shape)\n",
        "  return pmi_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Dataset, Build Vocab, and Compute PMI Matrix"
      ],
      "metadata": {
        "id": "2_Th-crZGViw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXcRuFV8jo0U"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n",
        "corpus = dataset[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOS_ejvNkhgY"
      },
      "outputs": [],
      "source": [
        "corpus = [example[\"text\"] for example in dataset[\"train\"] if example[\"text\"].strip() != \"\"]\n",
        "corpus = [(text, None) for text in corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klgrjJdOqqXd"
      },
      "outputs": [],
      "source": [
        "vocab = build_vocabulary(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svASj0gYsRf8"
      },
      "outputs": [],
      "source": [
        "co_matrix, word_counts, total = build_matrix(corpus, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbpM-I1esSq4"
      },
      "outputs": [],
      "source": [
        "pmi_matrix = build_pmi(co_matrix, word_counts, total, positive=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Drive & Save Matrix"
      ],
      "metadata": {
        "id": "MlzUqWkXGhPu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQJIyMFP3ela"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pmi_matrix = pmi_matrix.tocsr()\n",
        "save_npz('/content/drive/MyDrive/pmi_model5.npz', pmi_matrix)"
      ],
      "metadata": {
        "id": "Bplkjhe3Uahe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading and Using SVD"
      ],
      "metadata": {
        "id": "5zwgoiw4xx4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pmi_matrix = load_npz('/content/drive/MyDrive/pmi_model5.npz')"
      ],
      "metadata": {
        "id": "Gl0kyX_4uc3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svd = TruncatedSVD(n_components=300)  # or 100, 200, depending on RAM\n",
        "reduced_matrix = svd.fit_transform(pmi_matrix)\n",
        "print(reduced_matrix.shape)"
      ],
      "metadata": {
        "id": "0n5_UUrCVI7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Analogy Methods"
      ],
      "metadata": {
        "id": "dwt0cF1GGs16"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2t9v_Er3oey"
      },
      "outputs": [],
      "source": [
        "def get_vector(word, vocab, reduced_matrix):\n",
        "    \"\"\"Get the dense vector for a word.\"\"\"\n",
        "    if word not in vocab:\n",
        "        return None\n",
        "    return reduced_matrix[vocab[word]]\n",
        "\n",
        "def find_best_match_fast(query_vec, reduced_matrix, vocab, exclude=None):\n",
        "    reverse_vocab = {i: w for w, i in vocab.items()}\n",
        "\n",
        "    # Use sklearn's version here\n",
        "    similarities = cosine_similarity(query_vec.reshape(1, -1), reduced_matrix)[0]\n",
        "\n",
        "    if exclude:\n",
        "        exclude_indices = {vocab[word] for word in exclude if word in vocab}\n",
        "        for idx in exclude_indices:\n",
        "            similarities[idx] = -np.inf\n",
        "\n",
        "    best_idx = np.argmax(similarities)\n",
        "    return reverse_vocab[best_idx]\n",
        "\n",
        "def solve_analogy(a, b, c, vocab, reduced_matrix):\n",
        "    \"\"\"Solve analogy: a is to b as c is to ?\"\"\"\n",
        "    vec_a = get_vector(a, vocab, reduced_matrix)\n",
        "    vec_b = get_vector(b, vocab, reduced_matrix)\n",
        "    vec_c = get_vector(c, vocab, reduced_matrix)\n",
        "\n",
        "    if vec_a is None or vec_b is None or vec_c is None:\n",
        "      return \"One or more words not in vocabulary.\"\n",
        "\n",
        "    # vector arithmetic: b - a + c\n",
        "    query_vec = vec_b - vec_a + vec_c\n",
        "\n",
        "    # exclude input words from result\n",
        "    result = find_best_match_fast(query_vec, reduced_matrix, vocab, exclude={a, b, c})\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading & Pre-processing Analogy Dataset"
      ],
      "metadata": {
        "id": "2FqCdejdGzRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/questions-words.txt\""
      ],
      "metadata": {
        "id": "mjneTHwNZUMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_analogies_by_section(filepath):\n",
        "    sections = {}\n",
        "    current_section = None\n",
        "\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.startswith(\":\"):\n",
        "                current_section = line.strip()[2:].lower()\n",
        "                sections[current_section] = []\n",
        "            else:\n",
        "                words = line.strip().lower().split()\n",
        "                if len(words) == 4:\n",
        "                    sections[current_section].append(tuple(words))\n",
        "    return sections"
      ],
      "metadata": {
        "id": "f22p--7mQaaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_by_section(sections, vocab, reduced_matrix):\n",
        "    from collections import defaultdict\n",
        "\n",
        "    results = {}\n",
        "    total_correct = total_total = total_missing = 0\n",
        "\n",
        "    for section, analogies in tqdm(sections.items(), desc=\"Evaluating Sections\"):\n",
        "        correct = total = missing = 0\n",
        "        for a, b, c, expected in tqdm(analogies, desc=f\"{section}\", leave=False):\n",
        "            if any(w not in vocab for w in (a, b, c, expected)):\n",
        "                missing += 1\n",
        "                continue\n",
        "\n",
        "            vec_a = get_vector(a, vocab, reduced_matrix)\n",
        "            vec_b = get_vector(b, vocab, reduced_matrix)\n",
        "            vec_c = get_vector(c, vocab, reduced_matrix)\n",
        "            query_vec = vec_b - vec_a + vec_c\n",
        "\n",
        "            top = find_best_match_fast(query_vec, reduced_matrix, vocab, exclude=[a, b, c])\n",
        "            predicted = top if top else None\n",
        "\n",
        "            if predicted == expected:\n",
        "              correct += 1\n",
        "            total += 1\n",
        "\n",
        "        accuracy = correct / total if total else 0\n",
        "        results[section] = {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"correct\": correct,\n",
        "            \"total\": total,\n",
        "            \"missing\": missing\n",
        "        }\n",
        "        total_correct += correct\n",
        "        total_total += total\n",
        "        total_missing += missing\n",
        "\n",
        "    overall = {\n",
        "        \"accuracy\": total_correct / total_total if total_total else 0,\n",
        "        \"correct\": total_correct,\n",
        "        \"total\": total_total,\n",
        "        \"missing\": total_missing\n",
        "    }\n",
        "\n",
        "    return results, overall"
      ],
      "metadata": {
        "id": "BdlKaSVwQkbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sections = load_analogies_by_section(file_path)\n",
        "section_results, overall = evaluate_by_section(sections, vocab, reduced_matrix)\n",
        "\n",
        "# Print results\n",
        "for sec, stats in section_results.items():\n",
        "    print(f\"{sec:30s} - Accuracy: {stats['accuracy']:.4f} ({stats['correct']}/{stats['total']}) Missing: {stats['missing']}\")\n",
        "print(\"\\nOverall Accuracy:\")\n",
        "print(overall)"
      ],
      "metadata": {
        "id": "_pGxgzjoTNZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing on SimLex999"
      ],
      "metadata": {
        "id": "aq_522vC8MqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simlex_path = \"/content/drive/MyDrive/SimLex-999.txt\"\n",
        "simlex = pd.read_csv(simlex_path, sep='\\t')"
      ],
      "metadata": {
        "id": "GV_j686W4MHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_word(w):\n",
        "    return w.rsplit(\"-\", 1)[0].lower() if \"-\" in w else w.lower()\n",
        "\n",
        "def evaluate_words(df, vocab, reduced_matrix, score_name):\n",
        "    similarities = []\n",
        "    human_scores = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        w1, w2 = clean_word(row['word1']), clean_word(row['word2'])\n",
        "        score = float(row[score_col])\n",
        "\n",
        "        if w1 in vocab and w2 in vocab:\n",
        "            vec1 = reduced_matrix[vocab[w1]]\n",
        "            vec2 = reduced_matrix[vocab[w2]]\n",
        "            sim = vec1 @ vec2 / (norm(vec1) * norm(vec2))\n",
        "            similarities.append(sim)\n",
        "            human_scores.append(score)\n",
        "\n",
        "    if not similarities:\n",
        "        return None, 0\n",
        "\n",
        "    corr, _ = spearmanr(similarities, human_scores)\n",
        "    return corr, len(similarities)\n"
      ],
      "metadata": {
        "id": "m17RNf8w7tvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = simlex['POS'].unique()\n",
        "\n",
        "for pos in pos_tags:\n",
        "    subset = simlex[simlex['POS'] == pos]\n",
        "    corr, count = evaluate_words(subset, vocab, reduced_matrix, 'SimLex999')\n",
        "    if corr is not None:\n",
        "        print(f\"POS: {pos:<5} | Spearman: {corr:.4f} | Pairs used: {count}\")\n",
        "    else:\n",
        "        print(f\"POS: {pos:<5} | Not enough valid pairs in vocab.\")"
      ],
      "metadata": {
        "id": "npesuH_17wIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing on WordSim353"
      ],
      "metadata": {
        "id": "xx1Eq4us8Pa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://alfonseca.org/eng/research/pubs/ws353simrel.tar\n",
        "!tar -xf ws353simrel.tar"
      ],
      "metadata": {
        "id": "3Qg8-ArH8LqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim_df = pd.read_csv(\"wordsim353_sim_rel/wordsim_similarity_goldstandard.txt\", sep=\"\\t\", header=None, names=[\"word1\", \"word2\", \"score\"])\n",
        "rel_df = pd.read_csv(\"wordsim353_sim_rel/wordsim_relatedness_goldstandard.txt\", sep=\"\\t\", header=None, names=[\"word1\", \"word2\", \"score\"])"
      ],
      "metadata": {
        "id": "-381vKQx8yqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim_corr, sim_count = evaluate_words(sim_df, vocab, reduced_matrix, 'score')\n",
        "rel_corr, rel_count = evaluate_words(rel_df, vocab, reduced_matrix, 'score')\n",
        "\n",
        "print(f\"Similarity Set  → Spearman: {sim_corr:.4f} | Pairs used: {sim_count}\")\n",
        "print(f\"Relatedness Set → Spearman: {rel_corr:.4f} | Pairs used: {rel_count}\")"
      ],
      "metadata": {
        "id": "LPNzQU0V9KkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing More"
      ],
      "metadata": {
        "id": "TFqWBHQ8DiIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/vecto-ai/word-benchmarks/master/word-similarity/monolingual/en/rg-65.csv\n",
        "!wget https://raw.githubusercontent.com/vecto-ai/word-benchmarks/master/word-similarity/monolingual/en/men.csv\n",
        "!wget https://raw.githubusercontent.com/vecto-ai/word-benchmarks/master/word-similarity/monolingual/en/mc-30.csv\n",
        "!wget https://raw.githubusercontent.com/vecto-ai/word-benchmarks/master/word-similarity/monolingual/en/mturk-771.csv"
      ],
      "metadata": {
        "id": "P0zGg68pCh4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rg65 = pd.read_csv(\"rg-65.csv\")\n",
        "men = pd.read_csv(\"men.csv\")\n",
        "mc30 = pd.read_csv(\"mc-30.csv\")\n",
        "mturk771 = pd.read_csv(\"mturk-771.csv\")"
      ],
      "metadata": {
        "id": "9Ctpnx-dDrnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_pos(w):\n",
        "    return w.split(\"-\")[-1] if \"-\" in w else \"unk\"\n",
        "\n",
        "men[\"pos1\"] = men[\"word1\"].apply(extract_pos)\n",
        "men[\"pos2\"] = men[\"word2\"].apply(extract_pos)\n",
        "\n",
        "men_pos_matched = men[men[\"pos1\"] == men[\"pos2\"]].copy()\n",
        "men_pos_matched[\"pos\"] = men_pos_matched[\"pos1\"]"
      ],
      "metadata": {
        "id": "Wj7BG8KuGCqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(men)"
      ],
      "metadata": {
        "id": "3RYHSSmLGgx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = {\n",
        "    'RG-65': (rg65, 'similarity'),\n",
        "    'MEN': (men, 'similarity'),\n",
        "    'MC-30': (mc30, 'similarity'),\n",
        "    'MTurk-771': (mturk771, 'similarity')\n",
        "}\n",
        "\n",
        "for name, (df, score_col) in datasets.items():\n",
        "    corr, count = evaluate_words(df, vocab, reduced_matrix, score_col)\n",
        "    print(f\"{name}: Spearman correlation = {corr:.4f} on {count} pairs\")"
      ],
      "metadata": {
        "id": "ZGmpsDJlDs1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for pos_tag in men_pos_matched[\"pos\"].unique():\n",
        "    subset = men_pos_matched[men_pos_matched[\"pos\"] == pos_tag]\n",
        "    corr, count = evaluate_words(subset, vocab, reduced_matrix, 'similarity')\n",
        "    if corr is not None:\n",
        "        print(f\"POS: {pos_tag:<2} | Spearman: {corr:.4f} | Pairs: {count}\")\n",
        "    else:\n",
        "        print(f\"POS: {pos_tag:<2} | Not enough valid pairs.\")"
      ],
      "metadata": {
        "id": "UUt9PUSoGHod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cosine Neighbour Space"
      ],
      "metadata": {
        "id": "OB-DBeSMIKC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_neighbors(word, vocab, reduced_matrix, top_n=10):\n",
        "    if word not in vocab:\n",
        "        print(f\"'{word}' not in vocabulary.\")\n",
        "        return []\n",
        "\n",
        "    index = vocab[word]\n",
        "    vec = reduced_matrix[index]\n",
        "    if norm(vec) == 0:\n",
        "        print(f\"'{word}' has a zero vector.\")\n",
        "        return []\n",
        "\n",
        "    vec_norm = vec / norm(vec)\n",
        "\n",
        "    # Normalize entire matrix safely (avoid division by 0)\n",
        "    matrix_norms = np.linalg.norm(reduced_matrix, axis=1, keepdims=True)\n",
        "    matrix_norms[matrix_norms == 0] = 1\n",
        "    matrix_norm = reduced_matrix / matrix_norms\n",
        "\n",
        "    sims = matrix_norm @ vec_norm\n",
        "\n",
        "    # Use reverse vocab for fast lookup\n",
        "    reverse_vocab = {i: w for w, i in vocab.items()}\n",
        "\n",
        "    # Get sorted top indices, skipping the word itself\n",
        "    top_indices = sims.argsort()[::-1]\n",
        "    top_words = []\n",
        "    for i in top_indices:\n",
        "        if i == index:\n",
        "            continue\n",
        "        word_i = reverse_vocab.get(i)\n",
        "        if word_i:\n",
        "            top_words.append((word_i, sims[i]))\n",
        "        if len(top_words) == top_n:\n",
        "            break\n",
        "\n",
        "    return top_words"
      ],
      "metadata": {
        "id": "mFhpuE8LINM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neighbors = get_top_neighbors(\"brother\", vocab, reduced_matrix, top_n=5)\n",
        "for word, sim in neighbors:\n",
        "    print(f\"{word:<15} similarity: {sim:.4f}\")"
      ],
      "metadata": {
        "id": "nM8BiD2JIOP1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}